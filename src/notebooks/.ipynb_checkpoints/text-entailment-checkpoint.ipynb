{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99937e4-99b5-490c-a334-bce87b30db9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d139c77-fbde-4401-b991-7a2ebdaa2416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77fcd355-5a55-4fdc-bbc1-ce7f2a10ef4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (407047, 5)\n"
     ]
    }
   ],
   "source": [
    "df_processed = pd.read_csv(\"datasets/cleaned_dataset.csv\")\n",
    "print(f\"Dataset shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07fc8ff0-82d0-4dac-a48d-26a32db92106",
   "metadata": {},
   "outputs": [],
   "source": [
    "premises = df_processed['premise'].tolist()\n",
    "hypotheses = df_processed['hypothesis'].tolist()\n",
    "labels = df_processed['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286a7028-9ec8-4681-9447-9efe71116844",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp_premise, X_test_premise, X_temp_hypothesis, X_test_hypothesis, y_temp, y_test = train_test_split(\n",
    "    premises, hypotheses, labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "# Second split: train vs validation\n",
    "X_train_premise, X_val_premise, X_train_hypothesis, X_val_hypothesis, y_train, y_val = train_test_split(\n",
    "    X_temp_premise, X_temp_hypothesis, y_temp,\n",
    "    test_size=0.25,  # 0.25 * 0.8 = 0.2 of total data for validation\n",
    "    random_state=42,\n",
    "    stratify=y_temp )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5dd8c-04c5-4e26-bc41-944181e07308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bad50f5-72c4-45dd-87b9-6ffd3241d1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-21.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.12.15-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.7.0-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.4-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.3.2-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.20.1-cp311-cp311-win_amd64.whl.metadata (76 kB)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached aiohttp-3.12.15-cp311-cp311-win_amd64.whl (453 kB)\n",
      "Downloading multidict-6.6.4-cp311-cp311-win_amd64.whl (46 kB)\n",
      "Using cached yarl-1.20.1-cp311-cp311-win_amd64.whl (86 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached frozenlist-1.7.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Using cached propcache-0.3.2-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Using cached pyarrow-21.0.0-cp311-cp311-win_amd64.whl (26.2 MB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "  Attempting uninstall: fsspec\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------------ ---------------------  6/13 [dill]\n",
      "   ------------------ ---------------------  6/13 [dill]\n",
      "   ------------------------ ---------------  8/13 [yarl]\n",
      "   --------------------------- ------------  9/13 [multiprocess]\n",
      "   --------------------------- ------------  9/13 [multiprocess]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ---------------------------------------- 13/13 [datasets]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 pyarrow-21.0.0 xxhash-3.5.0 yarl-1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ff5b63-f46a-4565-afde-7faea42f2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dict = {\n",
    "    \"premises\": X_train_premise,\n",
    "    \"hypotheses\": X_train_hypothesis,\n",
    "    \"labels\": y_train\n",
    "}\n",
    "\n",
    "val_dict = {\n",
    "    \"premises\": X_val_premise,\n",
    "    \"hypotheses\": X_val_hypothesis,\n",
    "    \"labels\": y_val\n",
    "}\n",
    "\n",
    "test_dict = {\n",
    "    \"premises\": X_test_premise,\n",
    "    \"hypotheses\": X_test_hypothesis,\n",
    "    \"labels\": y_test\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eeaf83c-39f7-4997-aaff-8b0e1ba86e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premises': 'every single one of them is a tax-cutting, reform-the-government, conservative republican, gingrich declared on abc.', 'hypotheses': 'gingrich was made that he did not get a seat at the time.', 'labels': 1}\n",
      "{'premises': 'and uh so i had her baby sitting but she was six months pregnant and it was getting too much for her so i just quit i would rather quit and take care of my own kids than let somebody else raise them', 'hypotheses': 'my babysitter was approaching her third trimester and struggling so decided to look after my kids instead', 'labels': 0}\n",
      "{'premises': 'uh you can you can buy bags of silver coins a a bag has a thousand dollars face value in it and it is traded for silver', 'hypotheses': 'the bags are available for sale and you get them for just a thousand dollars.', 'labels': 0}\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset objects for each split\n",
    "train_ds = Dataset.from_dict(train_dict)\n",
    "val_ds = Dataset.from_dict(val_dict)\n",
    "test_ds = Dataset.from_dict(test_dict)\n",
    "\n",
    "# Combine into a DatasetDict for convenience\n",
    "ds = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": val_ds,\n",
    "    \"test\": test_ds\n",
    "})\n",
    "\n",
    "# Access individual splits\n",
    "print(ds[\"train\"][0])\n",
    "print(ds[\"validation\"][0])\n",
    "print(ds[\"test\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7c1fd29-8665-4a1c-b367-9ba9aa1e8ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 244227\n",
      "Validation samples: 81410\n",
      "Test samples: 81410\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train samples: {len(y_train)}\")\n",
    "print(f\"Validation samples: {len(y_val)}\")\n",
    "print(f\"Test samples: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dfe610-dd3b-4b47-927a-109e38213391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bb9dbad-174d-415b-8ba9-6ac0cef455ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "def preprocess_function(examples, batch_size=2):\n",
    "    inputs = [f\"Premise : {x} Hypothesis: {y} Label : \" for x, y in zip(examples[\"premises\"], examples[\"hypotheses\"])]\n",
    "    all_encodings = {'input_ids': [], 'attention_mask': []}\n",
    "    for i in range(0, len(examples[\"premises\"]), batch_size):\n",
    "        batch_input = inputs[i:i+batch_size]\n",
    "        model_inputs = tokenizer(batch_input)\n",
    "        for i in range(len(batch_input)):\n",
    "            sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "            model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "                max_length - len(sample_input_ids)\n",
    "            ) + sample_input_ids\n",
    "            model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "            ][i]\n",
    "            model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "            model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "            all_encodings[\"input_ids\"].append(model_inputs[\"input_ids\"])\n",
    "            all_encodings['attention_mask'].append(model_inputs['attention_mask'])\n",
    "    final_encodings = {\n",
    "        'input_ids': torch.cat(all_encodings['input_ids'], dim=0),\n",
    "        'attention_mask': torch.cat(all_encodings['attention_mask'], dim=0)\n",
    "    }\n",
    "    return final_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4fa8c6a-f7fd-45ea-91e5-df4cf2ccda59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train:   0%|                                                              | 0/244227 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "preprocess_function() missing 2 required positional arguments: 'hypotheses' and 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_encodings = \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTokenizing train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m val_encodings = ds[\u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m].map(\n\u001b[32m      9\u001b[39m     preprocess_function,\n\u001b[32m     10\u001b[39m     batched=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     desc=\u001b[33m\"\u001b[39m\u001b[33mTokenizing val\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m test_encodings = ds[\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m].map(\n\u001b[32m     16\u001b[39m     preprocess_function,\n\u001b[32m     17\u001b[39m     batched=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     desc=\u001b[33m\"\u001b[39m\u001b[33mTokenizing test\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\AI4SE\\GitHub_Projects\\text-entailment\\Lib\\site-packages\\datasets\\arrow_dataset.py:560\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m self_format = {\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    555\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    558\u001b[39m }\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\AI4SE\\GitHub_Projects\\text-entailment\\Lib\\site-packages\\datasets\\arrow_dataset.py:3318\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3316\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3317\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m3318\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3319\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3321\u001b[39m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\AI4SE\\GitHub_Projects\\text-entailment\\Lib\\site-packages\\datasets\\arrow_dataset.py:3674\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3672\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3673\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3674\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3676\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\AI4SE\\GitHub_Projects\\text-entailment\\Lib\\site-packages\\datasets\\arrow_dataset.py:3624\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3622\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3623\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3624\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\AI4SE\\GitHub_Projects\\text-entailment\\Lib\\site-packages\\datasets\\arrow_dataset.py:3547\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3545\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3546\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3547\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[31mTypeError\u001b[39m: preprocess_function() missing 2 required positional arguments: 'hypotheses' and 'tokenizer'"
     ]
    }
   ],
   "source": [
    "train_encodings = ds[\"train\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    load_from_cache_file=False,\n",
    "    remove_columns=ds[\"train\"].column_names,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "val_encodings = ds[\"validation\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    load_from_cache_file=False,\n",
    "    remove_columns=ds[\"validation\"].column_names,\n",
    "    desc=\"Tokenizing val\"\n",
    ")\n",
    "test_encodings = ds[\"test\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    load_from_cache_file=False,\n",
    "    remove_columns=ds[\"test\"].column_names,\n",
    "    desc=\"Tokenizing test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74a90b-6924-45cf-94ff-46cd212c9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = ds.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=ds[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
