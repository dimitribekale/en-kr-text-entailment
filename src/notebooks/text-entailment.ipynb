{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99937e4-99b5-490c-a334-bce87b30db9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d139c77-fbde-4401-b991-7a2ebdaa2416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77fcd355-5a55-4fdc-bbc1-ce7f2a10ef4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (407047, 5)\n"
     ]
    }
   ],
   "source": [
    "df_processed = pd.read_csv(\"datasets/cleaned_dataset.csv\")\n",
    "print(f\"Dataset shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07fc8ff0-82d0-4dac-a48d-26a32db92106",
   "metadata": {},
   "outputs": [],
   "source": [
    "premises = df_processed['premise'].tolist()\n",
    "hypotheses = df_processed['hypothesis'].tolist()\n",
    "labels = df_processed['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "286a7028-9ec8-4681-9447-9efe71116844",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp_premise, X_test_premise, X_temp_hypothesis, X_test_hypothesis, y_temp, y_test = train_test_split(\n",
    "    premises, hypotheses, labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "# Second split: train vs validation\n",
    "X_train_premise, X_val_premise, X_train_hypothesis, X_val_hypothesis, y_train, y_val = train_test_split(\n",
    "    X_temp_premise, X_temp_hypothesis, y_temp,\n",
    "    test_size=0.25,  # 0.25 * 0.8 = 0.2 of total data for validation\n",
    "    random_state=42,\n",
    "    stratify=y_temp )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5dd8c-04c5-4e26-bc41-944181e07308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16ff5b63-f46a-4565-afde-7faea42f2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dict = {\n",
    "    \"premises\": X_train_premise,\n",
    "    \"hypotheses\": X_train_hypothesis,\n",
    "    \"labels\": y_train\n",
    "}\n",
    "\n",
    "val_dict = {\n",
    "    \"premises\": X_val_premise,\n",
    "    \"hypotheses\": X_val_hypothesis,\n",
    "    \"labels\": y_val\n",
    "}\n",
    "\n",
    "test_dict = {\n",
    "    \"premises\": X_test_premise,\n",
    "    \"hypotheses\": X_test_hypothesis,\n",
    "    \"labels\": y_test\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eeaf83c-39f7-4997-aaff-8b0e1ba86e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premises': 'every single one of them is a tax-cutting, reform-the-government, conservative republican, gingrich declared on abc.', 'hypotheses': 'gingrich was made that he did not get a seat at the time.', 'labels': 1}\n",
      "{'premises': 'and uh so i had her baby sitting but she was six months pregnant and it was getting too much for her so i just quit i would rather quit and take care of my own kids than let somebody else raise them', 'hypotheses': 'my babysitter was approaching her third trimester and struggling so decided to look after my kids instead', 'labels': 0}\n",
      "{'premises': 'uh you can you can buy bags of silver coins a a bag has a thousand dollars face value in it and it is traded for silver', 'hypotheses': 'the bags are available for sale and you get them for just a thousand dollars.', 'labels': 0}\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset objects for each split\n",
    "train_ds = Dataset.from_dict(train_dict)\n",
    "val_ds = Dataset.from_dict(val_dict)\n",
    "test_ds = Dataset.from_dict(test_dict)\n",
    "\n",
    "# Combine into a DatasetDict for convenience\n",
    "ds = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": val_ds,\n",
    "    \"test\": test_ds\n",
    "})\n",
    "\n",
    "# Access individual splits\n",
    "print(ds[\"train\"][0])\n",
    "print(ds[\"validation\"][0])\n",
    "print(ds[\"test\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7c1fd29-8665-4a1c-b367-9ba9aa1e8ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 244227\n",
      "Validation samples: 81410\n",
      "Test samples: 81410\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train samples: {len(y_train)}\")\n",
    "print(f\"Validation samples: {len(y_val)}\")\n",
    "print(f\"Test samples: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74dfe610-dd3b-4b47-927a-109e38213391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bb9dbad-174d-415b-8ba9-6ac0cef455ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\n",
    "        f\"Premise: {p} Hypothesis: {h}\" \n",
    "        for p, h in zip(examples[\"premises\"], examples[\"hypotheses\"])\n",
    "    ]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    # Include labels in the output dictionary if available\n",
    "    model_inputs[\"labels\"] = examples[\"labels\"]  # or adjust key name if needed\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0136d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4fa8c6a-f7fd-45ea-91e5-df4cf2ccda59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d360c6d38b494816974a250fccb84e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train:   0%|          | 0/244227 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70d68d992d845d19cf8924a275212bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing val:   0%|          | 0/81410 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9f72f5d563448ea05e520ed007d603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing test:   0%|          | 0/81410 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_encodings = ds[\"train\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    load_from_cache_file=False,\n",
    "    remove_columns=ds[\"train\"].column_names,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "val_encodings = ds[\"validation\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    load_from_cache_file=False,\n",
    "    remove_columns=ds[\"validation\"].column_names,\n",
    "    desc=\"Tokenizing val\"\n",
    ")\n",
    "test_encodings = ds[\"test\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    load_from_cache_file=False,\n",
    "    remove_columns=ds[\"test\"].column_names,\n",
    "    desc=\"Tokenizing test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e74a90b-6924-45cf-94ff-46cd212c9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c88dd622",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_encodings, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n",
    "eval_dataloader = DataLoader(val_encodings, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79413d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PromptEncoderConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35925524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from peft) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from peft) (2.9.0.dev20250813+cu129)\n",
      "Requirement already satisfied: transformers in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from peft) (4.55.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from peft) (4.67.1)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Using cached accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from peft) (0.34.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.8.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from transformers->peft) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\bekal\\onedrive\\desktop\\ai4se\\github_projects\\text-entailment\\lib\\site-packages (from transformers->peft) (0.21.4)\n",
      "Downloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Using cached accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: accelerate, peft\n",
      "\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   ---------------------------------------- 2/2 [peft]\n",
      "\n",
      "Successfully installed accelerate-1.10.0 peft-0.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3aab9a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 822,275 || all params: 278,868,230 || trainable%: 0.2949\n"
     ]
    }
   ],
   "source": [
    "peft_config = PromptEncoderConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=20, encoder_hidden_size=128)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9035e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "lr = 1e-5\n",
    "num_epochs = 10\n",
    "dataset_size = 244227\n",
    "total_steps = num_epochs * (dataset_size // batch_size)\n",
    "warmup_steps = int(0.1 * total_steps)  # 10% warmup steps\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a856c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, \n",
    "        average='weighted', \n",
    "        zero_division=0\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        labels, predictions, \n",
    "        average=None, \n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'precision_per_class': precision_per_class.tolist(),\n",
    "        'recall_per_class': recall_per_class.tolist(),\n",
    "        'f1_per_class': f1_per_class.tolist(),\n",
    "        'support_per_class': support.tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5a8a6422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15265/15265 [26:12<00:00,  9.71it/s]\n",
      "100%|██████████| 5089/5089 [03:57<00:00, 21.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Eval loss: 1.0796, Train Loss: 1.0940337251804713 Accuracy: 0.4181, F1: 0.4109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15265/15265 [26:14<00:00,  9.70it/s]\n",
      "100%|██████████| 5089/5089 [03:56<00:00, 21.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Eval loss: 1.0840, Train Loss: 1.093210575383802 Accuracy: 0.3809, F1: 0.3160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15265/15265 [26:15<00:00,  9.69it/s]\n",
      "100%|██████████| 5089/5089 [03:59<00:00, 21.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Eval loss: 1.0861, Train Loss: 1.0924456885787959 Accuracy: 0.3702, F1: 0.2865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15265/15265 [26:16<00:00,  9.68it/s]\n",
      "100%|██████████| 5089/5089 [04:01<00:00, 21.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Eval loss: 1.0831, Train Loss: 1.091860765911812 Accuracy: 0.3835, F1: 0.3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15265/15265 [26:15<00:00,  9.69it/s]\n",
      "100%|██████████| 5089/5089 [03:59<00:00, 21.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Eval loss: 1.0824, Train Loss: 1.0919149455811756 Accuracy: 0.3883, F1: 0.3335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15265/15265 [26:16<00:00,  9.68it/s]\n",
      "100%|██████████| 5089/5089 [03:58<00:00, 21.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Eval loss: 1.0801, Train Loss: 1.0912611850784757 Accuracy: 0.3970, F1: 0.3533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15265/15265 [26:16<00:00,  9.68it/s]\n",
      "100%|██████████| 5089/5089 [03:58<00:00, 21.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Eval loss: 1.0812, Train Loss: 1.090796236120362 Accuracy: 0.3929, F1: 0.3454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15265/15265 [26:16<00:00,  9.68it/s]\n",
      "100%|██████████| 5089/5089 [03:58<00:00, 21.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Eval loss: 1.0792, Train Loss: 1.0905510258721478 Accuracy: 0.4013, F1: 0.3659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15265/15265 [26:15<00:00,  9.69it/s]\n",
      "100%|██████████| 5089/5089 [03:58<00:00, 21.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Eval loss: 1.0817, Train Loss: 1.0906518046324112 Accuracy: 0.3884, F1: 0.3332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15265/15265 [26:15<00:00,  9.69it/s]\n",
      "100%|██████████| 5089/5089 [03:58<00:00, 21.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Eval loss: 1.0820, Train Loss: 1.0903775750751226 Accuracy: 0.3868, F1: 0.3291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\"\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        eval_loss += outputs.loss.item()\n",
    "        all_logits.append(outputs.logits.detach().cpu().numpy())\n",
    "        all_labels.append(batch[\"labels\"].detach().cpu().numpy())\n",
    "\n",
    "    eval_loss /= len(eval_dataloader)\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    all_logits = np.concatenate(all_logits, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    metrics = compute_metrics((all_logits, all_labels))\n",
    "\n",
    "    train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Eval loss: {eval_loss:.4f}, Train Loss: {train_loss} Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be387f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-entailment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
